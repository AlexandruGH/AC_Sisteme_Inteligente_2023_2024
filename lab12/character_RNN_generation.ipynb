{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOrqYrAUscUg"
      },
      "source": [
        "![](https://i.imgur.com/eBRPvWB.png)\n",
        "\n",
        "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
        "\n",
        "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
        "\n",
        "```\n",
        "> python generate.py -n 500\n",
        "\n",
        "PAOLTREDN:\n",
        "Let, yil exter shis owrach we so sain, fleas,\n",
        "Be wast the shall deas, puty sonse my sheete.\n",
        "\n",
        "BAUFIO:\n",
        "Sirh carrow out with the knonuot my comest sifard queences\n",
        "O all a man unterd.\n",
        "\n",
        "PROMENSJO:\n",
        "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
        "An as shalp will of that seal think.\n",
        "\n",
        "NUKINUS:\n",
        "And house it to thee word off hee:\n",
        "And thou charrota the son hange of that shall denthand\n",
        "For the say hor you are of I folles muth me?\n",
        "```\n",
        "\n",
        "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPEaDOWfscUi"
      },
      "source": [
        "# Recommended Reading\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
        "\n",
        "* http://pytorch.org/ For installation instructions\n",
        "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
        "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
        "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
        "\n",
        "It would also be useful to know about RNNs and how they work:\n",
        "\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
        "\n",
        "Also see these related tutorials from the series:\n",
        "\n",
        "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
        "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlxurZhZswA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33b0dde-300d-42f1-b9e1-d17d63496318"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp2WDdeHscUl"
      },
      "source": [
        "# Prepare data\n",
        "\n",
        "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJTQxEEV1pdd",
        "outputId": "be27545f-bae9-40af-b3af-0f5b5d33bd03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGcnZ2l_scUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a614273e-2620-4bae-fe29-7f09e6dea12f"
      },
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(open('/content/Luceafarul.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 9868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3KQEGlmscU4"
      },
      "source": [
        "To make inputs out of this big string of data, we will be splitting it into chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1HidIcqscU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a3431c-6802-4c2e-af06-fe030da816f8"
      },
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ma,\n",
            "Si soarele e tatal meu,\n",
            "Iar noaptea-mi este muma;\n",
            "\n",
            "O, vin', odorul meu nespus,\n",
            "Si lumea ta o lasa;\n",
            "Eu sunt luceafarul de sus,\n",
            "Iar tu sa-mi fii mireasa.\n",
            "\n",
            "O, vin', in parul tau balai\n",
            "S-anin cununi de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIPl6zjescVB"
      },
      "source": [
        "# Build the Model\n",
        "\n",
        "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBqNgHKzscVC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size) # We use embeddings instead of one-hot encoding\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers) # RNN network\n",
        "        self.decoder = nn.Linear(hidden_size, output_size) # Linear output layer\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.rnn(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size) # we initialize the first hidden stat with zeros"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm6XbpxAscVK"
      },
      "source": [
        "# Inputs and Targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlENTpcgscVM"
      },
      "source": [
        "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHA3U0-6scVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc73797f-3c07-42e6-af41-4fb3d9fff724"
      },
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNWIq-GPscVS"
      },
      "source": [
        "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eBfcSCWscVT"
      },
      "source": [
        "def random_training_set():\n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAy1g7U3w13_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6f9fbe-7e9a-44a8-dc16-44e13a086de5"
      },
      "source": [
        "random_training_set()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([96, 38, 24, 21, 24, 74, 23, 94, 25, 10, 21, 10, 29, 14, 94, 13, 14, 94,\n",
              "         22, 10, 27, 16, 14, 10, 23, 96, 55, 14, 74, 24, 18, 94, 13, 30, 12, 14,\n",
              "         94, 31, 14, 10, 12, 30, 27, 18, 94, 22, 30, 21, 29, 14, 73, 96, 54, 18,\n",
              "         94, 29, 24, 10, 29, 10, 94, 21, 30, 22, 14, 10, 74, 23, 94, 24, 12, 14,\n",
              "         10, 23, 96, 39, 14, 94, 29, 18, 23, 14, 94, 24, 94, 28, 74, 10, 28, 12,\n",
              "         30, 21, 29, 14, 75, 63, 96, 96, 74, 94, 73, 73, 50, 73, 94, 14, 28, 29,\n",
              "         18, 94, 15, 27, 30, 22, 24, 28, 73, 94, 12, 30, 22, 94, 23, 30, 22, 10,\n",
              "         74, 23, 94, 31, 18, 28, 96, 56, 23, 94, 18, 23, 16, 14, 27, 94, 28, 14,\n",
              "         94, 10, 27, 10, 29, 10, 73, 96, 39, 10, 27, 10, 94, 25, 14, 94, 12, 10,\n",
              "         21, 14, 10, 94, 12, 14, 74, 10, 18, 94, 13, 14, 28, 12, 17, 18, 28, 96,\n",
              "         49, 74, 24, 18, 94, 22, 14, 27, 16, 14, 94, 23, 18, 12, 18, 24, 13, 10,\n",
              "         29, 10]),\n",
              " tensor([38, 24, 21, 24, 74, 23, 94, 25, 10, 21, 10, 29, 14, 94, 13, 14, 94, 22,\n",
              "         10, 27, 16, 14, 10, 23, 96, 55, 14, 74, 24, 18, 94, 13, 30, 12, 14, 94,\n",
              "         31, 14, 10, 12, 30, 27, 18, 94, 22, 30, 21, 29, 14, 73, 96, 54, 18, 94,\n",
              "         29, 24, 10, 29, 10, 94, 21, 30, 22, 14, 10, 74, 23, 94, 24, 12, 14, 10,\n",
              "         23, 96, 39, 14, 94, 29, 18, 23, 14, 94, 24, 94, 28, 74, 10, 28, 12, 30,\n",
              "         21, 29, 14, 75, 63, 96, 96, 74, 94, 73, 73, 50, 73, 94, 14, 28, 29, 18,\n",
              "         94, 15, 27, 30, 22, 24, 28, 73, 94, 12, 30, 22, 94, 23, 30, 22, 10, 74,\n",
              "         23, 94, 31, 18, 28, 96, 56, 23, 94, 18, 23, 16, 14, 27, 94, 28, 14, 94,\n",
              "         10, 27, 10, 29, 10, 73, 96, 39, 10, 27, 10, 94, 25, 14, 94, 12, 10, 21,\n",
              "         14, 10, 94, 12, 14, 74, 10, 18, 94, 13, 14, 28, 12, 17, 18, 28, 96, 49,\n",
              "         74, 24, 18, 94, 22, 14, 27, 16, 14, 94, 23, 18, 12, 18, 24, 13, 10, 29,\n",
              "         10, 78]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLrgVHN1scVc"
      },
      "source": [
        "# Evaluating\n",
        "\n",
        "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7vU-df-scVc"
      },
      "source": [
        "# Temperature is a variable that controls the randomness of the selection process from the multinomial distribution\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oq5_Wz3scVh"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk95yQDlscVj"
      },
      "source": [
        "A helper to print the amount of time passed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETziEbVscVj"
      },
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuB1uF8xscVq"
      },
      "source": [
        "The main training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrQJIN1wscVr"
      },
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data.item() / chunk_len"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7Kut6vscVw"
      },
      "source": [
        "Then we define the training parameters, instantiate the model, and start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YZ4TTXqrscVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bed09d-ea38-4435-c243-526e09052c6e"
      },
      "source": [
        "n_epochs = 20000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss = train(*random_training_set())\n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0m 13s (100 0%) 2.0272]\n",
            "Wh,\n",
            "Un astii measa-t ni ae n pume vina starini soste ni voi dea cate aralesa s-umatasa ce si de pe de  \n",
            "\n",
            "[0m 26s (200 1%) 1.9493]\n",
            "Wheaiariga , sarii ti nat luminda.\n",
            "\n",
            "Da mostrati\n",
            "Ma vula\n",
            "Pare intalun lunesa vorasareaga-u ,,Caci nea t \n",
            "\n",
            "[0m 39s (300 1%) 1.9518]\n",
            "Whi se truntor sire,\n",
            "Cupoaste lotos,\n",
            "Si se primisi dor luma,\n",
            "Eu nesa o sapa.\n",
            "\n",
            "\n",
            "O, nele a ademi fare,\n",
            "S \n",
            "\n",
            "[0m 52s (400 2%) 1.7209]\n",
            "Whascum osus,\n",
            "Caci ocheme,\n",
            "Eu got osumoar cu vrea,\n",
            "Nrepati\n",
            "Si padatati\n",
            "Si vinil fi schii s de sa serbi \n",
            "\n",
            "[1m 4s (500 2%) 1.6612]\n",
            "Whivele ea de aprinici in adea de-nde.\n",
            "\n",
            "O, vin', orizodri, ca sunt pe mine prepte-ntreapteapari vie co \n",
            "\n",
            "[1m 17s (600 3%) 1.7564]\n",
            "Whe, ma privesc sunta sa vina trivescat pe canca meorm sin soari su-mi poamne\n",
            "Si miepus,\n",
            "Iar cu gremur \n",
            "\n",
            "[1m 30s (700 3%) 1.1545]\n",
            "Whii tinici mai camant\n",
            "Si vrei trecul unde si treguri nu goate nicitos, lunemi de-oi tremuritor,\n",
            "Si do \n",
            "\n",
            "[1m 43s (800 4%) 1.5856]\n",
            "Whipzii\n",
            "Ei mariand num si ga-n codat\n",
            "Ce cu mea fere\n",
            "Sub dorbe.\n",
            "\n",
            "Dar intreand vinguralii cu trecu-mle.\n",
            " \n",
            "\n",
            "[1m 56s (900 4%) 1.8062]\n",
            "Whipti\n",
            "Si priverul mea neste de-ai mate,\n",
            "Si mai\n",
            "Si de maii balai\n",
            "Luceafat ale,\n",
            "Pe premuri iaranteii se \n",
            "\n",
            "[2m 9s (1000 5%) 1.5684]\n",
            "Whimand inacitoa se intreapate,\n",
            "El tie, carii ma vinea ce sedeun si de nasc astrut\n",
            "Un mari, ne-ntrain  \n",
            "\n",
            "[2m 23s (1100 5%) 1.5468]\n",
            "Whici ei mea tredea sororara de se noarte.\n",
            "\n",
            "\n",
            "\n",
            "Coansi sa dorbe?\n",
            "\n",
            "Din viri,\n",
            "Din locinic luce, vinei\n",
            "Cum  \n",
            "\n",
            "[2m 36s (1200 6%) 1.1352]\n",
            "Whici ei ari din a moarmea rine vinguri Caturi ca sa-mi te in josului cupti\n",
            "Sub fusansul nu sand cu oc \n",
            "\n",
            "[2m 50s (1300 6%) 1.1073]\n",
            "Whind veste se vis\n",
            "Un vin', trisi\n",
            "E aluri;\n",
            "\n",
            "Si luni mai suce, lui sunt de alea-n asti din cresul de ma \n",
            "\n",
            "[3m 3s (1400 7%) 1.6577]\n",
            "Whine.\n",
            "\n",
            "Usor de privireafarul de sa-mi la un adrasa decion sa-imi sungia.\n",
            "\n",
            "Cum el trei gumani,\n",
            "Ci soar \n",
            "\n",
            "[3m 17s (1500 7%) 1.3752]\n",
            "Whise lunoaptea si-n, luceafari\n",
            "Ve-o roduri Catande dragu-une pugate\n",
            "Si vin',\n",
            "Reiele a raza,\n",
            "Patrunde, \n",
            "\n",
            "[3m 30s (1600 8%) 1.1325]\n",
            "Whipie;\n",
            "\n",
            "Cum es\n",
            "Un maninte,\n",
            "Candriungerite...\n",
            "\n",
            "Dar in zarelu sanii ramai urmoartele\n",
            "\n",
            "- ,,Dar brate.\n",
            "\n",
            "I \n",
            "\n",
            "[3m 43s (1700 8%) 1.1488]\n",
            "Whipi,\n",
            "Dar chipare sa se fas\n",
            "Saruta,\n",
            "Dar acu-i asti mea de bratul saritor\n",
            "Marele pentarte...\n",
            "\n",
            "Din mang \n",
            "\n",
            "[3m 57s (1800 9%) 1.2800]\n",
            "Whipa,\n",
            "Si lungeri\n",
            "Ve nemurii ca ratacitori\n",
            "De cunostii.\n",
            "\n",
            "Pata;\n",
            "\n",
            "Nu ca vremul luceafar\n",
            "Ca doi batalde p \n",
            "\n",
            "[4m 10s (1900 9%) 1.1639]\n",
            "Whisi. Dorbe,\n",
            "Te-o rusc de sus merei sa vin cere din remne-as cu mipespre-n ceri stra,\n",
            "Dar inturau ma  \n",
            "\n",
            "[4m 23s (2000 10%) 1.1938]\n",
            "Whipa.\n",
            "\n",
            "Cau sa ma patrunde-n chaosus\n",
            "Intre premenin cere de menit din repalu --l vecinoarte,\n",
            "El el moa \n",
            "\n",
            "[4m 36s (2100 10%) 1.1147]\n",
            "Whipa.\n",
            "In cer\n",
            "Hypere,\n",
            "Nu o zancununtit el in umbi.\n",
            "\n",
            "Si pe azi, braturat pe-o razii,\n",
            "Dara pe luceafar b \n",
            "\n",
            "[4m 49s (2200 11%) 1.4453]\n",
            "Whind sa se vrei chim,\n",
            "Viminii razi, sfinta in coate-aceli\n",
            "Se nici emor\n",
            "Num nimnii negaritor\n",
            "Si din sa \n",
            "\n",
            "[5m 2s (2300 11%) 1.1003]\n",
            "Whine teparu-n a se fii cate intoarte zile.\n",
            "\n",
            "Si vede a de apu-ntele-n gang\n",
            "Sa-ntre asai e da-mi in chi \n",
            "\n",
            "[5m 16s (2400 12%) 1.3640]\n",
            "Whic,\n",
            "Si tot pe vis\n",
            "Un asim nici te sal mai sfora pe curmene cu stiu un sunt amai\n",
            "S-an avele-i mun nu  \n",
            "\n",
            "[5m 30s (2500 12%) 1.2060]\n",
            "Whici sa nu greul negrei pe se-ntinde-mi, un hipi de urma sine\n",
            "Uimirea clunat cu pedee\n",
            "Si, ca-n jos,,\n",
            " \n",
            "\n",
            "[5m 44s (2600 13%) 1.4293]\n",
            "Whima-n valuri.\"\n",
            "\n",
            "- ,,O, esti mandra,\n",
            "Caci reci\n",
            "\"\n",
            "- ,,Daceti\n",
            "Si lasa,\n",
            "Ce tine,\n",
            "Si lungi se tine\n",
            "Luceaf \n",
            "\n",
            "[5m 58s (2700 13%) 1.4738]\n",
            "Whi de sus,\n",
            "Iubirea cum nici imparati\n",
            "Luceste colta de rumand iub de razi cu bratul te copile cu totul \n",
            "\n",
            "[6m 12s (2800 14%) 1.2219]\n",
            "Whine ma margine-ncepteat de parere.\n",
            "\n",
            "Si treie luminea-n vecind in urma\n",
            "Dar cenia se-lu stai -\n",
            "Cand de \n",
            "\n",
            "[6m 25s (2900 14%) 1.1611]\n",
            "Whi sa lui merge nicioi se stai l-au alunge numii mea meu de ca tatalina.\n",
            "\n",
            "Si poameri imple guritor\n",
            "Si \n",
            "\n",
            "[6m 38s (3000 15%) 1.1839]\n",
            "Whic si ingrai niste mimi,\n",
            "Dara de mii de alti\n",
            "Ce nu-Ceapta\n",
            "Si din un si si din apa.\n",
            "\n",
            "Ea duri?\n",
            "De sufl \n",
            "\n",
            "[6m 52s (3100 15%) 0.8880]\n",
            "Whipe?\n",
            "\n",
            "Parerea-n tranzlunu?.\n",
            "El nemura in lume,\n",
            "El vin', ma pacat ca dau esteaptea-n ziul meu felegi  \n",
            "\n",
            "[7m 5s (3200 16%) 1.5397]\n",
            "Whic si trept-aruma.\"\n",
            "\n",
            "- ,,Ce mandru taie?..\n",
            "\n",
            "Scant\n",
            "Sa-ntinea de manar ea vocul la vort,\n",
            "Si pe crezii, \n",
            "\n",
            "[7m 19s (3300 16%) 0.9816]\n",
            "Whic bu s-opri sa tremuritoare dau glasa,\n",
            "Dar cunoscut mor\n",
            "Ca cuprinde imple goartea\n",
            "\n",
            "Cand ramand sarm \n",
            "\n",
            "[7m 32s (3400 17%) 1.2189]\n",
            "Whi sa-ti din sferelesi soare-e piet ase;\n",
            "\n",
            "Ca de umele\n",
            "Lu voi scantei\n",
            "Cand aluma,\n",
            "Si de stiu esti se f \n",
            "\n",
            "[7m 45s (3500 17%) 1.3123]\n",
            "Whi pe inalta sti de stepe tu ma pacuni orizon nisa te-ntreagare,\n",
            "De pe umbra a luceafarul de supta pe \n",
            "\n",
            "[7m 59s (3600 18%) 1.2846]\n",
            "Whatar boteri Catalin?\n",
            "\n",
            "DSe reu se-o rulce,\n",
            "Iar lumi da-mi in acu\n",
            "Ca sa-mi pacator,\n",
            "Sa mea pe locul lu \n",
            "\n",
            "[8m 12s (3700 18%) 1.2988]\n",
            "Whici esti vreu suprinde ea vorbe poni fi ce fat de-o raza,\n",
            "Pataritor\n",
            "Si vrea-ntreapa fat fi stele pe  \n",
            "\n",
            "[8m 25s (3800 19%) 1.2551]\n",
            "Whipa,\n",
            "Si sa-mi deparle.\"\n",
            "\n",
            "- ,,Dargintii crestele-i soare,\n",
            "Dara ferea mea una.\n",
            "\n",
            "Parerea un lucepe.\"\n",
            "\n",
            "- \n",
            "\n",
            "[8m 38s (3900 19%) 1.3867]\n",
            "Whine dor spre opre-ncur una.\"\n",
            "\n",
            "Hyperion pe urmara pe urmate sa-mi idecat pe nici ea le prise chaos... \n",
            "\n",
            "[8m 51s (4000 20%) 1.0435]\n",
            "Whici amand, esti din casa.\n",
            "\n",
            "Dar de-ai marea mea de parerele\n",
            "\n",
            "- ,,Corice-l pinata veme\n",
            "Si nu treaptul. \n",
            "\n",
            "[9m 4s (4100 20%) 1.4534]\n",
            "Whi sti pe tot deschi de-oi iubi,\n",
            "Darand pe-o sarte,\n",
            "Si impara;\n",
            "\n",
            "Darmene sort,\n",
            "Da teeste ce cau a muna \n",
            "\n",
            "[9m 18s (4200 21%) 1.2592]\n",
            "Whimi\n",
            "Sa-ti de bulce-uceafarul nemuri\n",
            "C roti\n",
            "Seni vremi\n",
            "Dumb,\n",
            "Ca sa nu meu\n",
            "Si stele.\n",
            "\n",
            "Si-i somnca in j \n",
            "\n",
            "[9m 31s (4300 21%) 1.2621]\n",
            "Whipa,\n",
            "Sa pricepe.\n",
            "\n",
            "O, vina,\n",
            "Sa si tremandru si mai numai vremeni,\n",
            "Vedes otii.\n",
            "\n",
            "Eu numa,\n",
            "Ca sa nu semu \n",
            "\n",
            "[9m 44s (4400 22%) 1.1807]\n",
            "Whi sa te nu-l pulce,\n",
            "Cu plonil ea mea sa te stingere\n",
            "Si toti al draie;\n",
            "\n",
            "Cum de si-a dor,\n",
            "Si cadei chi \n",
            "\n",
            "[9m 58s (4500 22%) 1.4319]\n",
            "Whind pe vezmor de margarsa sase-l mai trid\n",
            "Tremurate nimic,\n",
            "Mai lina!\n",
            "\n",
            "Ci plant\n",
            "Si pleste feresc si i \n",
            "\n",
            "[10m 11s (4600 23%) 1.0884]\n",
            "Whi pe vreaja sa-i aot pamand pe cum ez vezi ma ducele giulge-n coruma,\n",
            "Si din apai\n",
            "Se larea,\n",
            "Patrunde \n",
            "\n",
            "[10m 25s (4700 23%) 0.9773]\n",
            "Whie\n",
            "Luceacar cufle,\n",
            "Pe nic,\n",
            "Dar in urmate ce nimirele nimic,\n",
            "Tu tarinte,\n",
            "Un chaostiu dot scara-i se i \n",
            "\n",
            "[10m 38s (4800 24%) 1.0702]\n",
            "Whica-n marea ce frumoasa catarg,\n",
            "Un margind foc senint ase culcerupuca\n",
            "\n",
            "-a pas\n",
            "A-mparasi sara pe cale \n",
            "\n",
            "[10m 52s (4900 24%) 1.2494]\n",
            "Whide draga mari stiit\n",
            "Si-am numai lulunecand pe-aprerand\n",
            "Si tanar boator de trece,\n",
            "Colo-ntrunde-n cas \n",
            "\n",
            "[11m 5s (5000 25%) 1.0912]\n",
            "Whicate si pas culce de cuprinde intoara idei\n",
            "Canini,\n",
            "Pe farmec din pe prigonil\n",
            "Si ochii,\n",
            "Din ganduris \n",
            "\n",
            "[11m 19s (5100 25%) 1.2228]\n",
            "Whi destelea pe gura.\"\n",
            "\n",
            "- ,,T'ginst simant\n",
            "Si dor de cer azi mai vremuri\n",
            "Raspi din oglinistiunt lu est \n",
            "\n",
            "[11m 32s (5200 26%) 1.0675]\n",
            "Whi in zadar\n",
            "Si trupu-i se prisi lasa;\n",
            "El veci\n",
            "Va vanatat cu bal\n",
            "O greaca lumineaza cumind in ganduri  \n",
            "\n",
            "[11m 46s (5300 26%) 1.1785]\n",
            "Whici negre dau mansupra ta pot de stele,\n",
            "Si se-nce-n oranditor\n",
            "M-argori\n",
            "Si de tare;\n",
            "\n",
            "Dar minil vezi m \n",
            "\n",
            "[11m 59s (5400 27%) 1.3073]\n",
            "Whi cupra de mori nu-o razi ce nu strasa,\n",
            "Si ma calea al in gand cu coroate cania fugi de stai,\n",
            "Dare t \n",
            "\n",
            "[12m 12s (5500 27%) 1.0612]\n",
            "Whatul meu ne prebu-mea ta a-n paj cu patatori\n",
            "Dar unde-n cer ade marg,\n",
            "Din la pari de-ai indrasa,\n",
            "Si  \n",
            "\n",
            "[12m 26s (5600 28%) 0.8379]\n",
            "Whiceara.\"\n",
            "\n",
            "- ,,Ce-tul tete-mi,\n",
            "Rebate-l meu se rumb\n",
            "Si se iaraci tu mindat el de vrea intalatori.\"\n",
            "\n",
            "- \n",
            "\n",
            "[12m 39s (5700 28%) 1.0823]\n",
            "Whici cunele chipe.\n",
            "\n",
            "Iar nisor\n",
            "Cu ochiulung si mulunti\n",
            "De nu-l patila trecintul,\n",
            "Ca stinea\n",
            "It\n",
            "Cand e t \n",
            "\n",
            "[12m 52s (5800 28%) 1.4917]\n",
            "Whetea mea de stele pe apra tatate?\n",
            "Ia ce-n marg lasa-n asta estepe negari\n",
            "De-ntand in mantrea ta desf \n",
            "\n",
            "[13m 5s (5900 29%) 1.0644]\n",
            "Wheaga si sus,\n",
            "Si tu lasa;\n",
            "Rasa-n jos...\n",
            "Si duce.\n",
            "\n",
            "Vremntul;\n",
            "\n",
            "Si mine.\n",
            "\n",
            "Din uzvantul de lu erbor noste \n",
            "\n",
            "[13m 19s (6000 30%) 1.3221]\n",
            "Whansi easui\n",
            "Si elesc noapta a sufle priveni,\n",
            "Privedei oaptace masa,\n",
            "\n",
            "Un cer de-ai par de treceritor\n",
            "P \n",
            "\n",
            "[13m 32s (6100 30%) 1.6210]\n",
            "Whipte -\n",
            "Paluratalii tu draga in aste,\n",
            "Caci esti cu mandri sa te-n faca\n",
            "Si de-nchivea cu dureafarea ta \n",
            "\n",
            "[13m 45s (6200 31%) 1.0841]\n",
            "Whang\n",
            "Sa te imple datimi ma mandata,\n",
            "Stor,\n",
            "Pe-al ntinte,\n",
            "Si cade-i tot dau taca nimica\n",
            "\n",
            "- ,,Cand sa-ti \n",
            "\n",
            "[13m 59s (6300 31%) 1.5387]\n",
            "Whicu-nde pieroare;\n",
            "\n",
            "Rin se nascuti\n",
            "Lucesti casti vede ele.\"\n",
            "\n",
            "- ,,Ce privinar,\n",
            "Vatal,\n",
            "Cacii,\n",
            "Dar naste \n",
            "\n",
            "[14m 12s (6400 32%) 1.3431]\n",
            "Whemesuca\n",
            "Si vremurand,\n",
            "I-au ase,\n",
            "Mai ca din pe tamenit cu ase nimore ndai valuri;\n",
            "\n",
            "Dat\n",
            "Intoare?\n",
            "Ampii \n",
            "\n",
            "[14m 26s (6500 32%) 1.2304]\n",
            "Wheralin cer aleu\n",
            "Si manar cot de sus\n",
            "Uimire scand e mai si toapta iopalacu-mi nemuritor,\n",
            "Dar inara la \n",
            "\n",
            "[14m 39s (6600 33%) 1.1735]\n",
            "Wher\n",
            "Si rasare\n",
            "Si, Hypei\n",
            "Rasarul de stele.\n",
            "\n",
            "Cumeie-ntreaga te-acele-i aci in genintii si dus..\n",
            "\n",
            "Si din \n",
            "\n",
            "[14m 53s (6700 33%) 1.0672]\n",
            "Whid\n",
            "Luminic\n",
            "De line se vreie,\n",
            "Eu te crecul meu se-ntind la deasa tefel pie dorul,\n",
            "Ci stele decu-t' de \n",
            "\n",
            "[15m 7s (6800 34%) 1.5077]\n",
            "Wh-alei c-orge,\n",
            "A-mpasa;\n",
            "Eu se-ntoarde braje durere lu-mea mea meniri pe arzezajunt lesa;\n",
            "Ca sorul;\n",
            "\n",
            "D \n",
            "\n",
            "[15m 20s (6900 34%) 1.6113]\n",
            "Whimin\n",
            "In sine dor de craininil\n",
            "Si lui cu greau\n",
            "In si gasa-n vede cau lung se numani sa sa-ntrea, fuce \n",
            "\n",
            "[15m 34s (7000 35%) 1.4810]\n",
            "Whare ce foarez-o sede pameric,\n",
            "Si oful de ince prea,\n",
            "Iri intoarsii\n",
            "El ochios, necu-i geni sa-mi impre \n",
            "\n",
            "[15m 48s (7100 35%) 1.5769]\n",
            "Whand poi ramai\n",
            "O orucati\n",
            "Sub sfort si min,\n",
            "Dar cum fi miie.\n",
            "\n",
            "Iar cer de apamanil de marica,\n",
            "Sar sorol \n",
            "\n",
            "[16m 1s (7200 36%) 1.1940]\n",
            "Wherand\n",
            "Si urma,\n",
            "Si nu-n palat\n",
            "-n vin' vecinici naste.\n",
            "\n",
            "Din lomna,\n",
            "Se-nsorzatargit el din focul nu vor \n",
            "\n",
            "[16m 15s (7300 36%) 1.3038]\n",
            "Whare privoare nu sin urma ducelor te-l manimi.\"\n",
            "\n",
            "\n",
            "arain, tatalininii-a priste ce pot din piere,\n",
            "Si se \n",
            "\n",
            "[16m 28s (7400 37%) 1.2378]\n",
            "Whare ea voi moartea ramai\n",
            "O goamenii camargi din cer\n",
            "Hyperios,\n",
            "Dar ulbind ca Catargi,\n",
            "De stii calese\n",
            " \n",
            "\n",
            "[16m 41s (7500 37%) 1.3818]\n",
            "Whata ma caleaga luceafar bland,\n",
            "Alunecar -\n",
            "O, dulga ce lung si din urma sa-l ma deza ceralii capul, r \n",
            "\n",
            "[16m 55s (7600 38%) 1.3022]\n",
            "Wherind pe tatai\n",
            "Si mari sa seu mea cuprand ochiul tare;\n",
            "\n",
            "Dt' veci\n",
            "De trupte, lucet: ei asul meu soare \n",
            "\n",
            "[17m 8s (7700 38%) 1.1884]\n",
            "Whenia la pas pe cere nasti\n",
            "In lm-n luceafar bltele cumintelege.\"\n",
            "\n",
            "- ,,Din lume;\n",
            "\n",
            "Din lin coate-n gata \n",
            "\n",
            "[17m 21s (7800 39%) 1.4354]\n",
            "Whele...\n",
            "Hypre sfelta de soare;\n",
            "\n",
            "In aducate-altri sa manici eu far num izvor nemuri si inchide draga t \n",
            "\n",
            "[17m 34s (7900 39%) 1.3167]\n",
            "Whopringe.\"\n",
            "\n",
            "- ,,Das doi sa si spre se de stii ca ei stelegi se nascut\n",
            "Un cern\n",
            "Si sintii\n",
            "Ca sus,\n",
            "Resan \n",
            "\n",
            "[17m 47s (8000 40%) 1.3354]\n",
            "Whipt cu ochii mari noapta-nchide dragalat pe-ntind cu parul meu saniul te-nt si in greu sa-ntind sa i \n",
            "\n",
            "[18m 1s (8100 40%) 1.0973]\n",
            "Whendat in gand dau goasemere;\n",
            "\n",
            "Si-n vise-ti pe plin\n",
            "In castii\n",
            "Si ochii miscatoare;\n",
            "\n",
            "O, luceafarul, al \n",
            "\n",
            "[18m 14s (8200 41%) 1.4653]\n",
            "Wherul de stand ru esta te marea pe urma un Vede leschindea pe ii deci inalana sa nemacii,\n",
            "Patrunde-n  \n",
            "\n",
            "[18m 28s (8300 41%) 1.8531]\n",
            "Whinic si grece iarmec sarostrale o lon luceafami\n",
            "Sub recultiti nunt in arz-o revin,\n",
            "Spce ste cuboapai \n",
            "\n",
            "[18m 41s (8400 42%) 1.5879]\n",
            "Whecubirea-\n",
            "Si totilorelestile?\"\n",
            "\n",
            "\n",
            "reca si cum ni trev,\n",
            "Eu in vang\n",
            "Catare,\n",
            "Eu toti\n",
            "In veci numele in f \n",
            "\n",
            "[18m 55s (8500 42%) 1.4900]\n",
            "Wheratul ta Crita de se-om flinurinte,\n",
            "Si de sunt lung de sunt loc din urman\n",
            "Ce trea mea as de adevara \n",
            "\n",
            "[19m 8s (8600 43%) 1.4774]\n",
            "Whisusi,\n",
            "Dar apunte,\n",
            "Pris\n",
            "De mareapa de marginio,\n",
            "Si indrind voi nu valul as de-ai cu parus,\n",
            "Ma pas pr \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1wc9eA-scV2"
      },
      "source": [
        "# Plotting the Training Losses\n",
        "\n",
        "Plotting the historical loss from all_losses shows the network learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvoZA01DscV2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-u8k-g_scV-"
      },
      "source": [
        "# Evaluating at different \"temperatures\"\n",
        "\n",
        "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
        "\n",
        "We can see the effects of this by adjusting the `temperature` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqfL2NaDscV_"
      },
      "source": [
        "print(evaluate('Pe sufletele ', 200, temperature=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DVvunJ3scWC"
      },
      "source": [
        "Lower temperatures are less varied, choosing only the more probable outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsaxOutsscWE"
      },
      "source": [
        "print(evaluate('Th', 200, temperature=0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Ia83D8scWJ"
      },
      "source": [
        "Higher temperatures more varied, choosing less probable outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3EiVNSoscWJ"
      },
      "source": [
        "print(evaluate('Th', 200, temperature=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "LMTIFjLdscWS"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "* Train with your own dataset, e.g.\n",
        "    * Text from another author\n",
        "    * Blog posts\n",
        "    * Code\n",
        "* Increase number of layers and network size to get better results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8VNfhMVscWT"
      },
      "source": [
        "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmDrciIEuK-X"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}